{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset creation with pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we'll save a dataset in parquet with pyspark and version the metadata inside Verta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "  .master(\"local\") \\\n",
    "  .appName(\"parquet_example\") \\\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we read a local csv and save it out as parquet. Here we're saving to local disk, but we could have saved to HDFS or S3 instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('census-train.csv', header=True, inferSchema=True)\n",
    "df.repartition(5).write.mode('overwrite').parquet('datasets/census-train-parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use pyspark to read information about the dataset back to us and create a Path dataset with all the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path Version\n",
      "    /Users/conrado/workspace/modeldb/demos/census/datasets/census-train-parquet/part-00000-aa276e29-cd0e-4208-bead-14fec5a13149-c000.snappy.parquet\n",
      "        57199 bytes\n",
      "        last modified: 2021-01-29 12:29:51.080000\n",
      "        MD5 checksum: 1f3201dadb184e7c0caed32c10c61206\n",
      "    /Users/conrado/workspace/modeldb/demos/census/datasets/census-train-parquet/part-00001-aa276e29-cd0e-4208-bead-14fec5a13149-c000.snappy.parquet\n",
      "        57017 bytes\n",
      "        last modified: 2021-01-29 12:29:51.081000\n",
      "        MD5 checksum: 5d86421785794e1374bf7635065e4551\n",
      "    /Users/conrado/workspace/modeldb/demos/census/datasets/census-train-parquet/part-00002-aa276e29-cd0e-4208-bead-14fec5a13149-c000.snappy.parquet\n",
      "        57283 bytes\n",
      "        last modified: 2021-01-29 12:29:51.080000\n",
      "        MD5 checksum: 4230c0bec341695187e9c1371c6fddc7\n",
      "    /Users/conrado/workspace/modeldb/demos/census/datasets/census-train-parquet/part-00003-aa276e29-cd0e-4208-bead-14fec5a13149-c000.snappy.parquet\n",
      "        57102 bytes\n",
      "        last modified: 2021-01-29 12:29:51.080000\n",
      "        MD5 checksum: 8b6d7060111b85ca897e776b72fe629e\n",
      "    /Users/conrado/workspace/modeldb/demos/census/datasets/census-train-parquet/part-00004-aa276e29-cd0e-4208-bead-14fec5a13149-c000.snappy.parquet\n",
      "        56946 bytes\n",
      "        last modified: 2021-01-29 12:29:51.224000\n",
      "        MD5 checksum: 6d2c5315ade80af9274f463fcf962a38\n"
     ]
    }
   ],
   "source": [
    "# This cell will be replaced with the following in a future client version\n",
    "#\n",
    "# from verta.dataset import Path\n",
    "# path_dataset = Path.with_spark('datasets/census-train-parquet')\n",
    "\n",
    "rdd = sc.binaryFiles('datasets/census-train-parquet')\n",
    "\n",
    "# Create a Path metadta for each component\n",
    "def process_component(entry):\n",
    "    filepath, content = entry\n",
    "    filepath = filepath[len(\"file:\"):]\n",
    "    from verta.dataset import Path\n",
    "    return Path(filepath)\n",
    "\n",
    "# Then reduce by just doing the sum\n",
    "path_dataset = rdd.map(process_component).reduce(lambda a, b: a+b)\n",
    "print(path_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can save that metadata into Verta with a new dataset version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set email from environment\n",
      "set developer key from environment\n",
      "connection successfully established\n",
      "created new Dataset: Census parquet - pyspark example in workspace: p72-mi-data\n",
      "created new Dataset Version: 1 for Census parquet - pyspark example\n"
     ]
    }
   ],
   "source": [
    "from verta import Client\n",
    "\n",
    "client = Client(\"https://point72.app.verta.ai\")\n",
    "dataset = client.get_or_create_dataset(\"Census parquet - pyspark example\", workspace=\"p72-mi-data\")\n",
    "dataset_version = dataset.create_version(path_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
