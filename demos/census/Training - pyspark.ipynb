{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with pyspark\n",
    "In this example, we'll use a previously logged dataset to train a pyspark model and then log the model to Verta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset\n",
    "We'll load the daset previously logged into Verta. We only saved the metadata, so we assume the dataset is still there. We could also have used Verta managed datasets to create a snapshot for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set email from environment\n",
      "set developer key from environment\n",
      "connection successfully established\n",
      "got existing Dataset: Census parquet - pyspark example\n",
      "got existing dataset version: 7ec5763db7685e86c16f950815502d8a48be81bc6a5b8b1e8c3c638ca130bf50\n"
     ]
    }
   ],
   "source": [
    "from verta import Client\n",
    "client = Client(\"https://point72.app.verta.ai\")\n",
    "dataset = client.get_or_create_dataset(\"Census parquet - pyspark example\", workspace=\"p72-mi-data\")\n",
    "dataset_version = dataset.get_latest_version()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The we load the parquet files into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "dfs = map(spark.read.parquet, dataset_version.get_content().list_paths())\n",
    "df = functools.reduce(lambda a,b: a.union(b), dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And process the dataframe to create a dataframe that is compatible with SparkML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[x for x in df.columns if x != '>50k'],\n",
    "    outputCol='features')\n",
    "\n",
    "df = assembler.transform(df)\n",
    "df = df.withColumn(\"label\", col(\">50k\"))\n",
    "df = df[\"features\", \"label\"]\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "We'll just use a simple logistic regression to fit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: (43,[],[])\n",
      "Intercept: -1.1069221254176718\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "hyperparameters = {\n",
    "    \"maxIter\": 10,\n",
    "    \"regParam\": 0.3,\n",
    "    \"elasticNetParam\": 0.8,\n",
    "}\n",
    "\n",
    "lr = LogisticRegression(**hyperparameters)\n",
    "lrModel = lr.fit(df)\n",
    "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we log the artifact and metadata to Verta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created new Project: Census Income - pyspark example in workspace: p72-mi-data\n",
      "created new Experiment: Logistic regression\n",
      "created new ExperimentRun: Run 62604161195224355382\n",
      "uploading part 1\n",
      "upload complete (model)\n"
     ]
    }
   ],
   "source": [
    "# Create an experiment run, which is a model instance\n",
    "project = client.set_project(name=\"Census Income - pyspark example\", workspace=\"p72-mi-data\")\n",
    "experiment = client.set_experiment(name=\"Logistic regression\")\n",
    "run = client.set_experiment_run()\n",
    "\n",
    "# Log the hyperparameters\n",
    "run.log_hyperparameters(hyperparameters)\n",
    "\n",
    "# Save the model as an artifact\n",
    "lrModel.save(\"spark_model\")\n",
    "run.log_artifact(\"model\", \"spark_model\") # This will package the folder created as a zip\n",
    "\n",
    "# Link the dataset to the model\n",
    "run.log_dataset_version(\"training\", dataset_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
